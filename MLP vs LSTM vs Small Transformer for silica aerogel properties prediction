import pandas as pd
import numpy as np
import tensorflow as tf
import time
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, LayerNormalization, MultiHeadAttention, Reshape, Flatten
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# 파일 경로 설정
data_path = r"C:\Users\82102\OneDrive\바탕 화면\Pd sensor impedance data..xlsx"

# 데이터 로드
data = pd.read_excel(data_path, header=0)

# 데이터 전처리 함수
def preprocess_data(data):
    X = data["impedance"].values.reshape(-1, 1)
    y = data["Interpolated Surface area (m2/g)"].values.reshape(-1, 1)
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    return X_scaled, y_scaled, scaler_X, scaler_y

# 데이터 전처리
X_scaled, y_scaled, scaler_X, scaler_y = preprocess_data(data)

# 전처리된 데이터 확인
print(X_scaled[:5])
print(y_scaled[:5])

# 모델 정의 함수들
def create_mlp_model(input_shape):
    inputs = Input(shape=input_shape)
    dense = Dense(64, activation='relu')(inputs)
    output = Dense(1)(dense)
    model = Model(inputs, output)
    return model

def create_transformer_model(input_shape, num_heads=2, ff_dim=64):
    inputs = Input(shape=input_shape)
    reshape = Reshape((1, input_shape[0]))(inputs)
    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[0])(reshape, reshape)
    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)
    ff_output = Dense(ff_dim, activation='relu')(attention_output)
    ff_output = Dense(input_shape[0])(ff_output)
    ff_output = LayerNormalization(epsilon=1e-6)(ff_output)
    flatten = Flatten()(ff_output)
    outputs = Dense(1)(flatten)
    model = Model(inputs, outputs)
    return model

def create_rnn_model(input_shape, hidden_units=64):
    inputs = Input(shape=input_shape)
    lstm = LSTM(hidden_units, return_sequences=False)(inputs)
    dense = Dense(64, activation='relu')(lstm)
    output = Dense(1)(dense)
    model = Model(inputs, output)
    return model

# 모델 생성 및 컴파일 함수
def create_and_compile_model(model_type='mlp', input_shape=(1,)):
    if model_type == 'mlp':
        model = create_mlp_model(input_shape)
    elif model_type == 'transformer':
        model = create_transformer_model((input_shape[0], 1))
    elif model_type == 'rnn':
        model = create_rnn_model((input_shape[0], 1))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mean_squared_error', 'mae'])
    return model

# 모델 학습 및 평가 함수
def train_and_evaluate(model, X_train, y_train, X_test, y_test, evaluate_interval=10):
    test_loss_history = []
    start_time = time.time()
    for epoch in range(300):
        history = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_split=0.1, verbose=0)
        if epoch % evaluate_interval == 0 or epoch == 299:
            test_loss, test_mse, test_mae = model.evaluate(X_test, y_test, verbose=0)
            test_loss_history.append(history.history['val_loss'][0])
            print(f"Epoch {epoch+1}/{300}: Test Loss = {test_loss}")
    end_time = time.time()
    training_time = end_time - start_time
    predictions = model.predict(X_test)
    return test_loss_history, predictions, training_time

# MAPE 계산 함수
def calculate_mape(true_values, predicted_values):
    return np.mean(np.abs((true_values - predicted_values) / true_values)) * 100

# 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# 데이터 형상 변경 (Transformer와 RNN 모델용)
X_train_3d = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_3d = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# RNN 모델 학습 및 평가
model_rnn = create_and_compile_model('rnn', input_shape=(X_train_3d.shape[1], X_train_3d.shape[2]))
test_loss_history_rnn, predictions_rnn, training_time_rnn = train_and_evaluate(model_rnn, X_train_3d, y_train, X_test_3d, y_test)

# MLP 모델 학습 및 평가
model_mlp = create_and_compile_model('mlp', input_shape=(X_train.shape[1],))
test_loss_history_mlp, predictions_mlp, training_time_mlp = train_and_evaluate(model_mlp, X_train, y_train, X_test, y_test)

# Transformer 모델 학습 및 평가
model_transformer = create_and_compile_model('transformer', input_shape=(X_train.shape[1],))
test_loss_history_transformer, predictions_transformer, training_time_transformer = train_and_evaluate(model_transformer, X_train_3d, y_train, X_test_3d, y_test)

# 결과 저장
results = {
    'rnn': {
        'test_loss_history': test_loss_history_rnn,
        'predictions': predictions_rnn,
        'training_time': training_time_rnn
    },
    'mlp': {
        'test_loss_history': test_loss_history_mlp,
        'predictions': predictions_mlp,
        'training_time': training_time_mlp
    },
    'transformer': {
        'test_loss_history': test_loss_history_transformer,
        'predictions': predictions_transformer,
        'training_time': training_time_transformer
    }
}

with open('model_results.pkl', 'wb') as f:
    pickle.dump(results, f)

# 그래프 시각화 (Epoch 별 Validation Loss)
epochs = range(0, 301, 10)  # 0, 10, 20, ..., 300까지 31개

plt.figure(figsize=(12, 8))

# 모든 모델의 Validation Loss 비교
plt.plot(epochs, results['rnn']['test_loss_history'], label='RNN', marker='o')
plt.plot(epochs, results['mlp']['test_loss_history'], label='MLP', marker='o')
plt.plot(epochs, results['transformer']['test_loss_history'], label='Transformer', marker='o')
plt.title('Validation Loss Comparison')
plt.xlabel('Epochs')
plt.ylabel('Validation Loss')
plt.legend(fontsize=12)
plt.grid(True)

plt.tight_layout()
plt.show()
