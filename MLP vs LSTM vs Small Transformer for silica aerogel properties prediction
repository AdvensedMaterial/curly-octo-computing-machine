import pandas as pd
import numpy as np
import tensorflow as tf
import time
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, LayerNormalization, MultiHeadAttention, Reshape, Flatten
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# File path (macOS)
data_path = "/Users/Pd sensor impedance data..xlsx"

# Load data
data = pd.read_excel(data_path, header=0)

# Data preprocessing function
def preprocess_data(data, time_steps=10):
    X = data[["impedance", "Time", "Frequency"]].values
    y_volume = data["Interpolated Pore Volume (cm3/g)"].values.reshape(-1, 1)
    y_area = data["Interpolated Surface area (m2/g)"].values.reshape(-1, 1)
    y_diameter = data["Interpolated Pore diameter (nm)"].values.reshape(-1, 1)

    scaler_X = MinMaxScaler()
    scaler_y_volume = MinMaxScaler()
    scaler_y_area = MinMaxScaler()
    scaler_y_diameter = MinMaxScaler()

    X_scaled = scaler_X.fit_transform(X)
    y_scaled_volume = scaler_y_volume.fit_transform(y_volume)
    y_scaled_area = scaler_y_area.fit_transform(y_area)
    y_scaled_diameter = scaler_y_diameter.fit_transform(y_diameter)

    X_seq = []
    y_seq_volume = []
    y_seq_area = []
    y_seq_diameter = []

    for i in range(len(X_scaled) - time_steps):
        X_seq.append(X_scaled[i:i + time_steps])
        y_seq_volume.append(y_scaled_volume[i + time_steps])
        y_seq_area.append(y_scaled_area[i + time_steps])
        y_seq_diameter.append(y_scaled_diameter[i + time_steps])

    X_seq = np.array(X_seq)
    y_seq_volume = np.array(y_seq_volume)
    y_seq_area = np.array(y_seq_area)
    y_seq_diameter = np.array(y_seq_diameter)

    return X_seq, y_seq_volume, y_seq_area, y_seq_diameter, scaler_X, scaler_y_volume, scaler_y_area, scaler_y_diameter

# Data preprocessing
time_steps = 10  # Set timesteps for time series data
X_scaled, y_scaled_volume, y_scaled_area, y_scaled_diameter, scaler_X, scaler_y_volume, scaler_y_area, scaler_y_diameter = preprocess_data(data, time_steps)

# Split data
X_train, X_test, y_train_volume, y_test_volume = train_test_split(X_scaled, y_scaled_volume, test_size=0.2, random_state=42)
_, _, y_train_area, y_test_area = train_test_split(X_scaled, y_scaled_area, test_size=0.2, random_state=42)
_, _, y_train_diameter, y_test_diameter = train_test_split(X_scaled, y_scaled_diameter, test_size=0.2, random_state=42)

# Model definition functions
def create_mlp_model(input_shape):
    inputs = Input(shape=(input_shape[-1],))
    dense = Dense(64, activation='relu')(inputs)
    dense = Dense(64, activation='relu')(dense)  # Add hidden layer
    output = Dense(1)(dense)
    model = Model(inputs, output)
    return model

def create_transformer_model(input_shape, num_heads=2, ff_dim=64):
    inputs = Input(shape=input_shape)
    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[-1])(inputs, inputs)
    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)
    ff_output = Dense(ff_dim, activation='relu')(attention_output)
    ff_output = Dense(input_shape[-1])(ff_output)
    ff_output = LayerNormalization(epsilon=1e-6)(ff_output)
    flatten = Flatten()(ff_output)
    outputs = Dense(1)(flatten)
    model = Model(inputs, outputs)
    return model

def create_rnn_model(input_shape, hidden_units=64):
    inputs = Input(shape=input_shape)
    lstm = LSTM(hidden_units, return_sequences=False)(inputs)
    dense = Dense(64, activation='relu')(lstm)
    output = Dense(1)(dense)
    model = Model(inputs, output)
    return model

# Model creation and compilation function
def create_and_compile_model(model_type='mlp', input_shape=(10, 3)):
    if model_type == 'mlp':
        model = create_mlp_model((input_shape[0] * input_shape[1],))
    elif model_type == 'transformer':
        model = create_transformer_model(input_shape)
    elif model_type == 'rnn':
        model = create_rnn_model(input_shape)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mean_squared_error', 'mae'])
    return model

# Model training and evaluation function
def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=300, evaluate_interval=10):
    test_loss_history = []
    start_time = time.time()
    for epoch in range(epochs):
        history = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_split=0.1, verbose=0)
        if epoch % evaluate_interval == 0 or epoch == epochs - 1:
            test_loss = model.evaluate(X_test, y_test, verbose=0)
            test_loss_history.append(test_loss[0])  # Append the first value of test_loss
            print(f"Epoch {epoch+1}/300: Test Loss = {test_loss[0]}")
    end_time = time.time()
    training_time = end_time - start_time
    predictions = model.predict(X_test)
    return test_loss_history, predictions, training_time

# Model training and evaluation
results = {}

for target, y_train, y_test in zip(['volume', 'area', 'diameter'], [y_train_volume, y_train_area, y_train_diameter], [y_test_volume, y_test_area, y_test_diameter]):
    results[target] = {}
    
    model_rnn = create_and_compile_model('rnn', input_shape=(time_steps, 3))
    test_loss_history_rnn, predictions_rnn, training_time_rnn = train_and_evaluate(model_rnn, X_train, y_train, X_test, y_test)
    
    model_mlp = create_and_compile_model('mlp', input_shape=(time_steps, 3))
    test_loss_history_mlp, predictions_mlp, training_time_mlp = train_and_evaluate(model_mlp, X_train.reshape(X_train.shape[0], -1), y_train, X_test.reshape(X_test.shape[0], -1), y_test)
    
    model_transformer = create_and_compile_model('transformer', input_shape=(time_steps, 3))
    test_loss_history_transformer, predictions_transformer, training_time_transformer = train_and_evaluate(model_transformer, X_train, y_train, X_test, y_test)
    
    results[target]['rnn'] = {
        'test_loss_history': test_loss_history_rnn,
        'predictions': predictions_rnn,
        'training_time': training_time_rnn
    }
    results[target]['mlp'] = {
        'test_loss_history': test_loss_history_mlp,
        'predictions': predictions_mlp,
        'training_time': training_time_mlp
    }
    results[target]['transformer'] = {
        'test_loss_history': test_loss_history_transformer,
        'predictions': predictions_transformer,
        'training_time': training_time_transformer
    }

with open('model_results.pkl', 'wb') as f:
    pickle.dump(results, f)

# Graph visualization function
def plot_results(results, metric, ylabel, title, save_path, property_name):
    models = ['rnn', 'mlp', 'transformer']
    colors = ['blue', 'orange', 'green']
    bar_width = 0.2

    plt.figure(figsize=(12, 8))

    index = np.arange(len(models))

    for i, model in enumerate(models):
        if metric == 'test_loss_history':
            values = [results[prop][model][metric][-1] for prop in results]
        else:
            values = [results[prop][model][metric] for prop in results]

        plt.bar(index + i * bar_width, values, bar_width, label=model, color=colors[i])

    plt.xlabel('Models', fontsize=16)
    plt.ylabel(ylabel, fontsize=16)
    plt.title(f'{title} ({property_name})', fontsize=20)
    plt.xticks(index + bar_width, [prop.capitalize() for prop in results.keys()], fontsize=14)
    plt.yticks(fontsize=14)
    plt.legend(fontsize=14)
    plt.grid(True)
    plt.tight_layout()

    # Save graph to desktop
    plt.savefig(save_path)
    plt.show()


