import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# 1. 데이터 준비
data = pd.read_csv('impedance_data.csv')
X = data.drop('label', axis=1).values
y = data['label'].values

# 데이터 정규화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. GAN 모델 정의
latent_dim = 100
input_shape = X_scaled.shape[1]

# 생성기 정의
def build_generator(latent_dim, output_shape):
    model = Sequential()
    model.add(Dense(128, activation='relu', input_dim=latent_dim))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(256, activation='relu'))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(512, activation='relu'))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(output_shape, activation='tanh'))
    return model

# 판별기 정의
def build_discriminator(input_shape):
    model = Sequential()
    model.add(Dense(512, input_dim=input_shape, activation=LeakyReLU(alpha=0.2)))
    model.add(Dropout(0.4))
    model.add(Dense(256, activation=LeakyReLU(alpha=0.2)))
    model.add(Dropout(0.4))
    model.add(Dense(1, activation='sigmoid'))
    return model

# 모델 빌드
generator = build_generator(latent_dim, input_shape)
discriminator = build_discriminator(input_shape)

# 판별기 컴파일
discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# GAN 모델 빌드
discriminator.trainable = False
gan = Sequential([generator, discriminator])

# GAN 컴파일
gan.compile(loss='binary_crossentropy', optimizer='adam')

# 3. GAN 학습
def train_gan(generator, discriminator, gan, data, epochs=10000, batch_size=128, latent_dim=100):
    half_batch = int(batch_size / 2)
    for epoch in range(epochs):
        # 진짜 데이터 샘플
        idx = np.random.randint(0, data.shape[0], half_batch)
        real_data = data[idx]
        
        # 가짜 데이터 샘플
        noise = np.random.normal(0, 1, (half_batch, latent_dim))
        fake_data = generator.predict(noise)
        
        # 판별기 학습
        d_loss_real = discriminator.train_on_batch(real_data, np.ones((half_batch, 1)))
        d_loss_fake = discriminator.train_on_batch(fake_data, np.zeros((half_batch, 1)))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
        
        # 생성기 학습
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        valid_y = np.ones((batch_size, 1))
        g_loss = gan.train_on_batch(noise, valid_y)
        
        # 학습 진행 상황 출력
        if epoch % 1000 == 0:
            print(f"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]")

# GAN 학습 시작
train_gan(generator, discriminator, gan, X_scaled)

# 4. 새로운 데이터 생성
def generate_data(generator, latent_dim, n_samples):
    noise = np.random.normal(0, 1, (n_samples, latent_dim))
    generated_data = generator.predict(noise)
    return generated_data

new_data = generate_data(generator, latent_dim, 100)
new_data_rescaled = scaler.inverse_transform(new_data)

# 5. 원본 데이터와 증강된 데이터 결합
original_data_with_labels = pd.DataFrame(X_scaled, columns=data.columns[:-1])
original_data_with_labels['label'] = data['label']

generated_data_with_labels = pd.DataFrame(new_data_rescaled, columns=data.columns[:-1])
generated_data_with_labels['label'] = 0  # 생성된 데이터의 라벨을 지정

combined_data = pd.concat([original_data_with_labels, generated_data_with_labels], ignore_index=True)

# 6. 결합된 데이터셋을 사용하여 분류 모델 학습
X_combined = combined_data.drop('label', axis=1).values
y_combined = combined_data['label'].values

X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)

# 분류 모델 정의
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])

# 모델 학습
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# 모델 평가
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy}')

# 7. 결과 시각화
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()
